{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunar Lander: Model Vs Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modify Scripts\n",
    "2. Make a parser to store data\n",
    "3. Analyse Data\n",
    "4. Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib import read_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Testing Scripts\n",
    "### Task1 : Machine Learning States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./lunar_lander_ml_states_player.py\", line 390, in <module>\n",
      "    env.render()\n",
      "  File \"./lunar_lander_ml_states_player.py\", line 359, in render\n",
      "    return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
      "  File \"/home/user1/anaconda3/envs/LunarLander/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\", line 105, in render\n",
      "    self.window.flip()\n",
      "  File \"/home/user1/anaconda3/envs/LunarLander/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\", line 506, in flip\n",
      "    self.context.flip()\n",
      "  File \"/home/user1/anaconda3/envs/LunarLander/lib/python3.5/site-packages/pyglet/gl/xlib.py\", line 359, in flip\n",
      "    glx.glXSwapBuffers(self.x_display, self.glx_window)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python ./lunar_lander_ml_states_player.py #> ml_states_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 : Machine Learning Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2018-04-29 21:51:10.545848: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-04-29 21:51:10.545961: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-04-29 21:51:10.545978: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-04-29 21:51:10.545990: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "85 -465.76199296\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./lunar_lander_ml_images_player.py\", line 403, in <module>\n",
      "    env.render()\n",
      "  File \"./lunar_lander_ml_images_player.py\", line 359, in render\n",
      "    return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
      "  File \"/home/user1/anaconda3/envs/LunarLander/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\", line 105, in render\n",
      "    self.window.flip()\n",
      "  File \"/home/user1/anaconda3/envs/LunarLander/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\", line 506, in flip\n",
      "    self.context.flip()\n",
      "  File \"/home/user1/anaconda3/envs/LunarLander/lib/python3.5/site-packages/pyglet/gl/xlib.py\", line 359, in flip\n",
      "    glx.glXSwapBuffers(self.x_display, self.glx_window)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./lunar_lander_ml_images_player.py #> ml_images_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3 : Deep Learning States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-04-29 21:45:40.572860: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-04-29 21:45:40.572967: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-04-29 21:45:40.573001: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-04-29 21:45:40.573014: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -272.385, steps: 207\n",
      "Episode 2: reward: -389.487, steps: 216\n",
      "Episode 3: reward: -227.794, steps: 142\n",
      "Episode 4: reward: -346.911, steps: 252\n",
      "Episode 5: reward: -415.479, steps: 184\n"
     ]
    }
   ],
   "source": [
    "! python ./lunar_lander_dl_player.py #> dl_states_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           Reward       Steps\n",
       " count    2.000000    2.000000\n",
       " mean   229.954493  251.000000\n",
       " std      7.045527    1.414214\n",
       " min    224.972553  250.000000\n",
       " 25%    227.463523  250.500000\n",
       " 50%    229.954493  251.000000\n",
       " 75%    232.445464  251.500000\n",
       " max    234.936434  252.000000,            Reward       Steps\n",
       " count    2.000000    2.000000\n",
       " mean   229.954493  251.000000\n",
       " std      7.045527    1.414214\n",
       " min    224.972553  250.000000\n",
       " 25%    227.463523  250.500000\n",
       " 50%    229.954493  251.000000\n",
       " 75%    232.445464  251.500000\n",
       " max    234.936434  252.000000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = ['ml_states_results.txt', \n",
    "         'ml_images_results.txt', \n",
    "         'dl_states_results.txt'\n",
    "        ]\n",
    "\n",
    "read_test_results(files[0]).describe(),\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Times\n",
    "Train times depend on a number of key factors.\n",
    "- how abstract is the representation being learned. - \"States\" or \"Images\"\n",
    "- What is representation being learned from? - Expert Player, Reinforcement\n",
    "\n",
    "When an effective abstract representations can be built, the model can learn to respond on a high level. Relationships between features and output i.e. the decision boundaries should be more simple to learn. A Less complex model could be used, improving fit time and memory complexity. The tradeoff is that state representations must be constructed apriori and they may not be extendable to other problems.\n",
    "\n",
    "A low level representation such as image data is more complex, there are highly conditional and non-linear relationship between features and output.\n",
    "This requires greater computational power and time to learn this representation. Deep learning Architectures produced may be in general more extensible to these image representations. Architectures that perform well on one image problem should perform well on another similar problem when trained.\n",
    "\n",
    "The next factor is the source of data. A limitation with models trained form expert data is that agents must exist to produce \"expert\" data. The model learns to copy specific actions its observed the expert execute rather than learning high level expert policies. If the problem conditions alter slightly, the model may be redundant, and requires more expert data in the new conditions.\n",
    "\n",
    "A reinforcment learner has the opprotunity to become and expert by interacting with the environment and trying different actions which maximise its reward. It learns strategies that provide value, a delayer reward. A model can learn from scratch, which can mean it can take a long time to train. It also is a more complex model as it learns how to interact with the environment and must then learn how those interactions dictate reward. The benefits are that an expert is not needed, the model may even produce unique expertise as it learns.\n",
    "\n",
    "The models used in the LunarLander task reflect this narrative. Learning form an expert is a fast way to train a decent model. ml_states and ml_images trained faster than dl_states. Implying expert data impacted train times more than the level of abstraction above image data. The ml_states player was faster than the ml_image player, illustrating the ovearhead of learning a complex low level representation.\n",
    "\n",
    "Architectures capable of effective reinforcment learning tend to be extendable to similar problems. Google's DeepMind atari team illustrate this.\n",
    "\n",
    "\n",
    "When comparing accuracy, the ml_states and images_player can copy expert behaviour a balanced accuracy rate of 90+, 70+. The ml state player can produce scores of 200+ indicating it has learned to solve the lunar lander task using methods it observed from expert.\n",
    "\n",
    "The Images player Has mixed scores, it crashes, missed the landing zone, but does not tend to consume excess fuel. It occasionally scores low positive values and values near 0. It is prown to failures -200. The learning curves of this model suggests that with more data, this could compete with the state player. Due to limitations with Colaboratory this model requires more memory efficient processing for image data. The dataset used should be loaded in batches from drive to memory for processing, allowing it to learn more.\n",
    "\n",
    "The reinforcement learner takes, time, it is annealed to try maximise its performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
